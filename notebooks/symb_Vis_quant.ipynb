{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2f454-3652-47bf-9568-094698857d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '/home/farnoush/symbolicXAI/')\n",
    "import numpy as np\n",
    "from symbxai.lrp.symbolic_xai import ViTSymbolicXAI\n",
    "from symbxai.model.vision_transformer import ModifiedViTForImageClassification\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "import torchvision\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# from lang_sam import LangSAM \n",
    "import pickle\n",
    "import itertools\n",
    "import torch\n",
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.ops import box_iou\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad65923-dfa9-4d7c-ab26-2c800afdaac0",
   "metadata": {},
   "source": [
    "## Extract eyes, mouth, and eyebrows segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef8dcb-6c88-41cf-a79f-9f77b6a973f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vision/eyes_segments.json', 'rb') as f:\n",
    "    eyes_file = pickle.load(f)\n",
    "\n",
    "with open('data/vision/eyebrows_segments.json', 'rb') as f:\n",
    "    eyebrows_file = pickle.load(f)\n",
    "\n",
    "with open('data/vision/mouth_segments.json', 'rb') as f:\n",
    "    mouth_file = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32d5ee-8a49-43cc-a70f-041b30c1b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(boxes, masks, logits, part='eyes'):\n",
    "    \"\"\"\n",
    "    Filter extracted bounding boxes based on their confidence scores and some other criteria.\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    if part == 'eyes':\n",
    "        num = 2\n",
    "    elif part == 'mouth':\n",
    "        num = 1\n",
    "    elif part == 'eyebrows':\n",
    "        num = 2\n",
    "\n",
    "    # If the mask is covering more than half of the image, remove it.\n",
    "    for i in range(len(logits)):\n",
    "        score = masks[i].sum() / (224 * 224)\n",
    "        if score > 0.5:\n",
    "            logits[i] = 0\n",
    "\n",
    "    indices = logits.argsort()[::-1]\n",
    "    values = logits[indices]\n",
    "    indices = indices[values != 0]\n",
    "    \n",
    "    if len(logits) > num:\n",
    "        indices = indices[:num]\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def split_image_into_patches(image, patch_size):\n",
    "    \"\"\"\n",
    "    Split an image into patches of a given size.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the image.\n",
    "    image_width, image_height = image.size\n",
    "\n",
    "    # Define the patch size.\n",
    "    patch_width, patch_height = patch_size\n",
    "\n",
    "    # Initialize a list to store patch bounding boxes.\n",
    "    patch_bounding_boxes = []\n",
    "\n",
    "    # Loop through the image and extract patches.\n",
    "    for y in range(0, image_height, patch_height):\n",
    "        for x in range(0, image_width, patch_width):\n",
    "            # Define the bounding box coordinates.\n",
    "            left = x\n",
    "            upper = y\n",
    "            right = x + patch_width\n",
    "            lower = y + patch_height\n",
    "\n",
    "            # Append the bounding box to the list.\n",
    "            patch_bounding_boxes.append((left, upper, right, lower))\n",
    "\n",
    "            # Extract the patch.\n",
    "            patch = image.crop((left, upper, right, lower))\n",
    "\n",
    "    return patch_bounding_boxes\n",
    "\n",
    "def find_intersecting_patches(box, patches):\n",
    "    \"\"\"\n",
    "    Check if there is an intersection between a bounding box and a patch.\n",
    "    \"\"\"\n",
    "    # Convert box and patches to tensor\n",
    "    box_tensor = torch.tensor([box], dtype=torch.float32)\n",
    "    patches_tensor = torch.tensor(patches, dtype=torch.float32)\n",
    "\n",
    "    # Calculate IoU (Intersection over Union) between the box and patches\n",
    "    iou = box_iou(box_tensor, patches_tensor)\n",
    "\n",
    "    # Find indices where IoU is non-zero (indicating intersection)\n",
    "    intersecting_indices = torch.nonzero(iou.squeeze() > 0).squeeze().tolist()\n",
    "\n",
    "    return intersecting_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e3aea-8174-4f7c-a1e1-19d4775a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_eyes = []\n",
    "for dic in tqdm(eyes_file):\n",
    "    indices = filter_boxes(boxes=dic['boxes_eyes'], masks=dic['mask_eyes'], logits=dic['logits_eyes'], part='eyes')\n",
    "    if indices is not None:\n",
    "        boxes_eyes.append(np.expand_dims(dic['boxes_eyes'][indices], axis=0))\n",
    "    else:\n",
    "        boxes_eyes.append(dic['boxes_eyes'][indices])\n",
    "\n",
    "boxes_mouth = []\n",
    "for dic in tqdm(mouth_file):\n",
    "    indices = filter_boxes(dic['boxes_mouth'], dic['mask_mouth'], dic['logits_mouth'], 'mouth')\n",
    "    if indices is not None:\n",
    "        boxes_mouth.append(np.expand_dims(dic['boxes_mouth'][indices], axis=0))\n",
    "    else:\n",
    "        boxes_mouth.append(dic['boxes_mouth'][indices])\n",
    "\n",
    "boxes_eyebrows = []\n",
    "for dic in tqdm(eyebrows_file):\n",
    "    indices = filter_boxes(dic['boxes_eyebrows'], dic['mask_eyebrows'], dic['logits_eyebrows'], 'eyebrows')\n",
    "    if indices is not None:\n",
    "       boxes_eyebrows.append(np.expand_dims(dic['boxes_eyebrows'][indices], axis=0)) \n",
    "    else:\n",
    "        boxes_eyebrows.append(dic['boxes_eyebrows'][indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b67d7-6011-4f81-873b-d64f9a27c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "size = int((256 / 224) * crop_size)\n",
    "segmentation_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=size),\n",
    "    torchvision.transforms.CenterCrop(crop_size),\n",
    "])\n",
    "\n",
    "\n",
    "patch_size = 16\n",
    "image_patches = []\n",
    "for dic in tqdm(eyes_file):\n",
    "    path = dic['image'].replace('/home/farnoush/symbolicXAI/dataset/images/', '/Users/thomasschnake/Research/Projects/symbolic_xai/datasets/fer_images/' )\n",
    "    image_pil = Image.open(path).convert(\"RGB\")\n",
    "    sample = segmentation_transforms(image_pil)\n",
    "    patches = split_image_into_patches(sample, (16, 16))\n",
    "    image_patches.append(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1211bd-3291-4f55-9e7b-45dee6c46f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes_indices = []\n",
    "mouth_indices = []\n",
    "eyebrows_indices = []\n",
    "\n",
    "for i in tqdm(range(280)):\n",
    "    eyes = []\n",
    "    for box in boxes_eyes[i][0]:\n",
    "        eyes.extend(find_intersecting_patches(box, image_patches[i]))\n",
    "    eyes_indices.append(eyes)\n",
    "\n",
    "    eyebrows = []\n",
    "    for box in boxes_eyebrows[i][0]:\n",
    "        eyebrows.extend(find_intersecting_patches(box, image_patches[i]))\n",
    "    eyebrows_indices.append(eyebrows)\n",
    "\n",
    "    mouth = []\n",
    "    for box in boxes_mouth[i][0]:\n",
    "        mouth.append(find_intersecting_patches(box, image_patches[i]))\n",
    "    mouth_indices.append(mouth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce131a-b932-4a70-9a5f-40c8c24d6c31",
   "metadata": {},
   "source": [
    "After a thorough visual insepction, these images were found to be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafb7f5-b10a-45c9-ac95-c11e3d7ece3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_images = [\n",
    "    12,\n",
    "    15,\n",
    "    16,\n",
    "    17,\n",
    "    24,\n",
    "    253,\n",
    "    260,\n",
    "    263,\n",
    "    279,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    9,\n",
    "    13,\n",
    "    252\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc951c-5f3f-4548-99c9-fb3c5b0edbc2",
   "metadata": {},
   "source": [
    "## Use Symbolic XAI framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f13dc-8028-4ccf-9b34-c5bee2dcf9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "size = int((256 / 224) * crop_size)\n",
    "size = 224\n",
    "\n",
    "label_to_id = {'sad': 0, 'disgust': 1, 'angry': 2, 'neutral': 3, 'fear': 4, 'surprise': 5, 'happy': 6}\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=size, interpolation=3),\n",
    "    torchvision.transforms.CenterCrop(crop_size),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c03d04-489e-485d-8e56-f71b26aa61aa",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3919db-7575-4e97-8ad2-4a4efa7bef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = transformers.AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "model = transformers.AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "model.eval()\n",
    "\n",
    "model.vit.embeddings.patch_embeddings.requires_grad = False\n",
    "model.vit.embeddings.patch_embeddings.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.endswith('embed'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "pretrained_embeddings = model.vit.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63d459-0dc4-48c3-9a61-b79669f2c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 1  # There are 2 options {1, 2}\n",
    "\n",
    "data_rel_dic = {}\n",
    "ids_dic = {}  # A dictionary that stores the ids of images that are correctly classified and belong to 'sad' or 'happy' classes.\n",
    "num = 0\n",
    "total = 0\n",
    "\n",
    "for i, dic in enumerate(tqdm(eyes_file)):\n",
    "    path = dic['image'].replace('/home/farnoush/symbolicXAI/dataset/images/', '/Users/thomasschnake/Research/Projects/symbolic_xai/datasets/fer_images/' )   \n",
    "    label = label_to_id[dic['label']]\n",
    "    image_pil = Image.open(path).convert(\"RGB\")\n",
    "    sample = transforms(image_pil)\n",
    "\n",
    "    target = torch.eye(7, dtype=sample.dtype)[0]\n",
    "    # target[0] = -1\n",
    "\n",
    "    symbxai_vit = ViTSymbolicXAI(\n",
    "        model=model,\n",
    "        embeddings=pretrained_embeddings,\n",
    "        sample=sample.unsqueeze(0),\n",
    "        target=target\n",
    "    )\n",
    "\n",
    "    pred = model(sample.unsqueeze(0))['logits'].argmax()\n",
    "\n",
    "    # Only consider 'happy' and 'sad' classes.\n",
    "    if label in [0, 6] and i not in problematic_images:\n",
    "        total += 1\n",
    "        if label == pred.item():\n",
    "            ids_dic[num] = i\n",
    "            num += 1\n",
    "\n",
    "            if True:\n",
    "                eyes = list(np.array(eyes_indices[i]).flatten() + 1) + list(np.array(eyebrows_indices[i]).flatten() + 1) \n",
    "                mouth = list(np.array(mouth_indices[i]).flatten() + 1) \n",
    "                rest = list(set(symbxai_vit.node_domain) - set(mouth) - set(eyes))\n",
    "                \n",
    "                not_mouth = list(set(symbxai_vit.node_domain) - set(mouth))\n",
    "                not_eyes = list(set(symbxai_vit.node_domain) - set(eyes))\n",
    "                not_rest = list(set(symbxai_vit.node_domain) - set(rest))\n",
    "                \n",
    "                if mode == 1:\n",
    "                    # Eyes\n",
    "                    eyes_relevance = symbxai_vit.symb_or(eyes) + symbxai_vit.symb_or(not_mouth) - symbxai_vit.symb_or(eyes + not_mouth)\n",
    "        \n",
    "                    # Mouth\n",
    "                    mouth_relevance = symbxai_vit.symb_or(not_eyes) + symbxai_vit.symb_or(mouth) - symbxai_vit.symb_or(not_eyes + mouth)\n",
    "        \n",
    "                    # Eyes and Mouth\n",
    "                    eyes_and_mouth_relevance = symbxai_vit.symb_or(eyes) + symbxai_vit.symb_or(mouth) - symbxai_vit.symb_or(eyes + mouth)\n",
    "        \n",
    "                    # Rest\n",
    "                    rest_relevance = symbxai_vit.subgraph_relevance(rest)\n",
    "    \n",
    "                elif mode == 2:\n",
    "                    # Eyes\n",
    "                    eyes_relevance = symbxai_vit.subgraph_relevance(eyes)\n",
    "        \n",
    "                    # Mouth\n",
    "                    mouth_relevance = symbxai_vit.subgraph_relevance(mouth)\n",
    "        \n",
    "                    # Eyes and Mouth\n",
    "                    eyes_and_mouth_relevance = symbxai_vit.symb_or(eyes) + symbxai_vit.symb_or(mouth) - symbxai_vit.symb_or(eyes + mouth)\n",
    "                    \n",
    "                    # Rest\n",
    "                    rest_relevance = symbxai_vit.symb_or(rest + not_eyes + not_mouth)\n",
    "        \n",
    "        \n",
    "                R_values = [eyes_relevance.item(), mouth_relevance.item(), eyes_and_mouth_relevance, rest_relevance.item()]\n",
    "    \n",
    "            data_rel_dic[i] = R_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa73f3-a9bf-4573-84e3-b30ad5d87f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745a888-9e9a-49f7-aaa6-ee2480df48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/farnoush/data_rel_sad_unnormalized_q1_generalized_gamma.pkl', 'wb') as f:\n",
    "    pickle.dump(data_rel_dic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b377389-ec84-4e4f-981e-82b3f1e8c935",
   "metadata": {},
   "source": [
    "Open saved relevance files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd516f-fc1d-4f6d-b047-a87fb20b5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open saved relavance values.\n",
    "data_len = 53\n",
    "X = np.zeros((data_len, 4, 2)) \n",
    "y = np.zeros((data_len))\n",
    "\n",
    "lbs = ['sad', 'happy']\n",
    "\n",
    "for i in range(2):\n",
    "    with open('data/vision/data_rel_{}_unnormalized_q1_generalized_gamma.pkl'.format(lbs[i]), 'rb') as f:\n",
    "        data_rel_dic = pickle.load(f)\n",
    "        print(len(data_rel_dic))\n",
    "\n",
    "    for idx, (key, data) in enumerate(data_rel_dic.items()):\n",
    "        label = label_to_id[eyebrows_file[key]['label']]\n",
    "        for  j in range(4):\n",
    "            X[idx, j, i] = data[j]\n",
    "        y[idx] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90433481-483f-481d-9ee3-753fdc955976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921bf73-6caf-4371-8c45-9ff676412819",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1bcb27-02fd-424f-8105-cd18aed3bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a75b6c-f6b6-4bdd-902f-fd297d9aa7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = 53\n",
    "if False:\n",
    "    X = np.zeros((data_len, 3)) \n",
    "    y = np.zeros((data_len))\n",
    "    \n",
    "    for idx, (key, data) in enumerate(data_rel_dic.items()):\n",
    "        label = label_to_id[eyebrows_file[key]['label']]\n",
    "        for  j in range(3):\n",
    "            X[idx, j] = data[j]\n",
    "        y[idx] = label\n",
    "\n",
    "X_hat = X[y == 0]\n",
    "y_hat = y[y == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff02519-eb2b-4646-b986-d1f715cfa9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_eyes = []\n",
    "for x in X_hat[:, 0]:\n",
    "    all_data_eyes.append(x)\n",
    "\n",
    "all_data_mouth = []\n",
    "for x in X_hat[:, 1]:\n",
    "    all_data_mouth.append(x)\n",
    "\n",
    "all_data_em = []\n",
    "for x in X_hat[:, 2]:\n",
    "    all_data_em.append(x)\n",
    "\n",
    "if False:\n",
    "    all_data_rest = []\n",
    "    for x in X_hat[:, 3]:\n",
    "        all_data_rest.append(x)\n",
    "\n",
    "# all_data = [np.array(all_data_eyes), np.array(all_data_mouth), np.array(all_data_em), np.array(all_data_rest)]\n",
    "all_data = [np.array(all_data_eyes), np.array(all_data_mouth), np.array(all_data_em)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1694f-9370-447a-ba93-cebb9e818357",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# ----- Eyes ------\n",
    "# Sad\n",
    "ys_neg_eyes = X[y == 0][:, 0, 1] - X[y == 0][:, 0, 0]\n",
    "xs_neg_eyes = np.random.normal(1, 0.1, ys_neg_eyes.shape[0])\n",
    "\n",
    "# Happy\n",
    "ys_pos_eyes = X[y == 6][:, 0, 1] - X[y == 6][:, 0, 0]\n",
    "xs_pos_eyes = np.random.normal(2, 0.1, ys_pos_eyes.shape[0])\n",
    "\n",
    "# ----- Mouth ------\n",
    "# Sad\n",
    "ys_neg_mouth = X[y == 0][:, 1, 1] - X[y == 0][:, 1, 0]\n",
    "xs_neg_mouth = np.random.normal(1, 0.1, ys_neg_mouth.shape[0])\n",
    "\n",
    "# Happy\n",
    "ys_pos_mouth = X[y == 6][:, 1, 1] - X[y == 6][:, 1, 0]\n",
    "xs_pos_mouth = np.random.normal(2, 0.1, ys_pos_mouth.shape[0])\n",
    "\n",
    "# ----- Eyes & Mouth ------\n",
    "# Sad\n",
    "ys_neg_em = X[y == 0][:, 2, 1] - X[y == 0][:, 2, 0]\n",
    "xs_neg_em = np.random.normal(1, 0.1, ys_neg_em.shape[0])\n",
    "\n",
    "# Happy\n",
    "ys_pos_em = X[y == 6][:, 2, 1] - X[y == 6][:, 2, 0]\n",
    "xs_pos_em = np.random.normal(2, 0.1, ys_pos_em.shape[0])\n",
    "\n",
    "\n",
    "# ----- Rest ------\n",
    "# Sad\n",
    "ys_neg_rest = X[y == 0][:, 3, 1] - X[y == 0][:, 3, 0]\n",
    "xs_neg_rest = np.random.normal(1, 0.1, ys_neg_em.shape[0])\n",
    "\n",
    "# Happy\n",
    "ys_pos_rest = X[y == 6][:, 3, 1] - X[y == 6][:, 3, 0]\n",
    "xs_pos_rest = np.random.normal(2, 0.1, ys_pos_em.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2972465-72bc-4c97-b5ca-1e8a8cc1989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, dpi=150, figsize=(18, 5))\n",
    "axs[0].scatter(xs_neg_eyes, ys_neg_eyes, alpha=.4, edgecolors='black', color='blue', s=120)\n",
    "axs[0].scatter(xs_pos_eyes, ys_pos_eyes, alpha=.4, edgecolors='black', color='red', s=120)\n",
    "# axs[0].set_title('$Eyes \\\\wedge \\\\neg{\\\\overline{Eyes}}$', size=25)\n",
    "axs[0].set_title('$Eyes \\\\wedge \\\\neg{Mouth}$', size=25)\n",
    "axs[0].axhline(y=0, xmin=0.05, xmax=0.95, color='black', ls='--', linewidth=1)\n",
    "axs[0].set_ylabel(\"$\\\\mathcal{A}(q,\\\\hat{f}^{\\\\text{h}}) - \\\\mathcal{A}(q,\\\\hat{f}^{\\\\text{s}})$\", size=25)\n",
    "axs[0].set_xticks([1, 2], ['-', '+'], size=20)\n",
    "axs[0].set_yticks([0])\n",
    "# axs[0].set_yscale('symlog')\n",
    "\n",
    "axs[1].scatter(xs_neg_mouth, ys_neg_mouth, alpha=.4, edgecolors='black', color='blue', s=120)\n",
    "axs[1].scatter(xs_pos_mouth, ys_pos_mouth, alpha=.4, edgecolors='black', color='red', s=120)\n",
    "# axs[1].set_title('$Mouth \\\\wedge \\\\neg{\\\\overline{Mouth}}$', size=25)\n",
    "axs[1].set_title('$Mouth \\\\wedge \\\\neg{Eyes}$', size=25)\n",
    "axs[1].axhline(y=0, xmin=0.05, xmax=0.95, color='black', ls='--', linewidth=1)\n",
    "axs[1].set_yscale('symlog')\n",
    "axs[1].set_xticks([1, 2], ['-', '+'], size=20)\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "axs[2].scatter(xs_neg_em, ys_neg_em, alpha=.4, edgecolors='black', color='blue', s=120)\n",
    "axs[2].scatter(xs_pos_em, ys_pos_em, alpha=.4, edgecolors='black', color='red', s=120)\n",
    "axs[2].set_title('$Eyes \\\\wedge Mouth$', size=25)\n",
    "axs[2].axhline(y=0, xmin=0.05, xmax=0.95, color='black', ls='--', linewidth=1)                                                \n",
    "# axs[2].set_yscale('symlog')\n",
    "axs[2].set_xticks([1, 2], ['-', '+'], size=20)\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "# axs[3].scatter(xs_neg_rest, ys_neg_rest, alpha=.4, edgecolors='black', color='blue', s=120)\n",
    "# axs[3].scatter(xs_pos_rest, ys_pos_rest, alpha=.4, edgecolors='black', color='red', s=120)\n",
    "# axs[3].set_title('$Rest$', size=25)\n",
    "# axs[3].axhline(y=0, xmin=0.05, xmax=0.95, color='black', ls='--', linewidth=1)                                                \n",
    "# # axs[2].set_yscale('symlog')\n",
    "# axs[3].set_xticks([1, 2], ['-', '+'], size=20)\n",
    "# axs[3].set_yticks([])\n",
    "\n",
    "# lim = np.array(list(axs[1]. get_ylim())) + np.array(list(axs[0].get_ylim())) + np.array(list(axs[2].get_ylim()))\n",
    "\n",
    "lim = None\n",
    "min_lim = axs[0].get_ylim()[0]\n",
    "max_lim = axs[0].get_ylim()[1]\n",
    "for i in range(1, 3):\n",
    "    l = axs[i].get_ylim()\n",
    "    if l[0] < min_lim:\n",
    "        min_lim = l[0]\n",
    "    if l[1] > max_lim:\n",
    "        max_lim = l[1]\n",
    "\n",
    "lim = np.array([min_lim, max_lim])\n",
    "print(lim)\n",
    "    \n",
    "axs[0].set_ylim(lim)\n",
    "axs[1].set_ylim(lim)\n",
    "axs[2].set_ylim(lim) \n",
    "# axs[3].set_ylim(lim)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0)\n",
    "fig.savefig(\"../pics/symbxai_vision_quant.svg\", facecolor=(1,1,1,0), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f2aa5-56fd-489c-9efe-297fe99a1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(4, 3), dpi=200)\n",
    "\n",
    "# plot box plot\n",
    "labels = ['Eyes', 'Mouth', 'Eyes & Mouth']\n",
    "bplot = axs.boxplot(\n",
    "    all_data,\n",
    "    labels=labels,\n",
    "    patch_artist=True)\n",
    "axs.set_title('Logit: \"Sad (-1)\", Images: \"Happy\"')\n",
    "\n",
    "# adding horizontal grid lines\n",
    "axs.yaxis.grid(True)\n",
    "axs.set_ylabel('Score')\n",
    "axs.set_xlabel('Query')\n",
    "\n",
    "colors = ['pink', 'lightblue', 'lightgreen']\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "plt.ylim(-150, 100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"/home/farnoush/symbolicXAI/figures/boxplot_sad_happy_2.png\", facecolor=(1,1,1,0), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70073e81-8403-43f8-a78e-91dcbfb93707",
   "metadata": {},
   "source": [
    "## Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5e6c3-a4fe-4eb9-9e70-28ecf55c33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "\n",
    "transforms1 = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "    torchvision.transforms.Resize(size=size, interpolation=3),\n",
    "    torchvision.transforms.CenterCrop(crop_size),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "sample_id = 275\n",
    "path = eyes_file[sample_id]['image']\n",
    "print(eyes_file[sample_id]['image'])\n",
    "sample1 = transforms1(Image.open(path).convert(\"RGB\"))\n",
    "patches = patchify(sample1.permute(1, 2, 0).numpy(), (16, 16, 3), step=16).squeeze(2)\n",
    "print(patches[0, 0].shape)\n",
    "\n",
    "fig, axs = plt.subplots(14, 14, figsize=(5, 5), dpi=150)\n",
    "\n",
    "for i in range(14):\n",
    "    for j in range(14):\n",
    "        axs[i, j].imshow(patches[i, j])\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.001, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870133f4-07dc-45c8-87bc-bc3418a258c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update({'hatch.linewidth': 0.5})\n",
    "cmap_neg = matplotlib.cm.get_cmap('Blues')\n",
    "cmap_pos = matplotlib.cm.get_cmap('Reds')\n",
    "\n",
    "red_color = (1,0.6,0.6)\n",
    "blue_color = (.6,.6,1)\n",
    "\n",
    "R_values = X[52, :, 0]\n",
    "sample_id = 277\n",
    "\n",
    "width=0.7\n",
    "alpha = 1\n",
    "labels = ['$Eyes \\\\wedge \\\\neg{Mouth}$', '$\\\\neg{Eyes} \\wedge Mouth$', '$Eyes \\\\wedge Mouth$', '$Rest$']\n",
    "# labels = ['$Eyes \\\\wedge \\\\neg{\\\\overline{Eyes}}$', '$Mouth \\\\wedge \\\\neg{\\\\overline{Mouth}}$', '$Eyes \\\\wedge Mouth$', '$Rest$']\n",
    "fig, ax = plt.subplots(1, figsize=(4, 1.5), dpi=200)\n",
    "for i, value in enumerate(R_values):\n",
    "    if i == 0:\n",
    "        label = 'Eyes'\n",
    "    elif i == 1:\n",
    "        label = 'Mouth'\n",
    "    elif i == 2:\n",
    "        label = 'Eyes & Mouth'\n",
    "    else:\n",
    "        label = 'Rest'\n",
    "            \n",
    "    if value < 0:\n",
    "        if label:\n",
    "            bars = ax.barh(i, value, alpha=alpha, edgecolor='black',\n",
    "                          color=blue_color, linewidth=0.5, label=label)\n",
    "        else:\n",
    "            bars = ax.barh(i, value, alpha=alpha, edgecolor='black',\n",
    "                          color=blue_color, linewidth=0.5)\n",
    "    else:\n",
    "        if label:\n",
    "            bars = ax.barh(i, value, alpha=alpha, edgecolor='black',\n",
    "                          color=red_color, linewidth=0.5, label=label)\n",
    "        else:\n",
    "            bars = ax.barh(i, value, alpha=alpha, edgecolor='black',\n",
    "                          color=red_color, linewidth=0.5)\n",
    "\n",
    "\n",
    "ax.set_yticks(np.arange(len(R_values)), labels, rotation=0)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_color('#DDDDDD')\n",
    "ax.tick_params(bottom=False, left=False)\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True, color='#EEEEEE')\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('')\n",
    "ax.set_facecolor(\"white\")\n",
    "\n",
    "if False:\n",
    "    legend = ax.legend(title='Context', title_fontsize=12, fontsize=12, loc=\"upper center\",\n",
    "                       bbox_to_anchor=(-.1, .9))\n",
    "    title = legend.get_title()\n",
    "    title.set_weight('bold')\n",
    "\n",
    "fig.show()\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"/home/farnoush/lang-segment-anything/symb_results/sample_{}_neg_logit.png\".format(sample_id), facecolor=(1,1,1,0), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda0180-c183-4df3-9b8b-74672cfaae8b",
   "metadata": {},
   "source": [
    "### First-Order LRP Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c48da3-3a7f-4654-80d4-8d1703276c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image, scale_cam_image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
    "import cv2\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        # self.model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2d4cd-8e92-4b33-ac3b-6ded649ef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = HuggingfaceToTensorModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7816d25-d97b-4c37-ae4c-b56d211e3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 275\n",
    "sample = transforms(Image.open(eyes_file[sample_id]['image']).convert(\"RGB\"))\n",
    "label = 6\n",
    "target = torch.eye(7, dtype=sample.dtype)[label]\n",
    "\n",
    "symbxai_vit = ViTSymbolicXAI(\n",
    "        model=model,\n",
    "        embeddings=pretrained_embeddings,\n",
    "        sample=sample.unsqueeze(0),\n",
    "        target=target\n",
    "    )\n",
    "\n",
    "R = symbxai_vit.node_relevance(mode='node@input').detach().numpy()[1:]\n",
    "\n",
    "normalized_R = ((R / np.linalg.norm(R)) * 255).clip(0, 255).astype(np.uint8)\n",
    "scores = scale_cam_image(normalized_R.reshape(1, 14, 14), (224, 224))\n",
    "\n",
    "rgb_img = np.float32(Image.open(eyes_file[sample_id]['image']).convert(\"RGB\"))\n",
    "rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10), dpi=150)\n",
    "h_map  = show_cam_on_image(rgb_img, scores[0], use_rgb=True)\n",
    "axs.imshow(h_map)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"/home/farnoush/lang-segment-anything/lrp_heatmaps/sample_{}.png\".format(sample_id), facecolor=(1,1,1,0), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d350908-6fdb-4c87-a0eb-97cb5d3f18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(R > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388f57f-92e8-4da9-bcf5-0cb7538e6c17",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28789b2-03c9-42ba-a84c-66de12a9897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def compute_best_mapping(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    return np.transpose(np.asarray(linear_sum_assignment(w.max() - w))), w\n",
    "    \n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    mapping, w = compute_best_mapping(y_true, y_pred)\n",
    "    return np.array([w[i, j] for i, j in mapping]).sum() * 1.0 / y_pred.size, mapping, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2043ac3-c408-4512-9f0b-22ed2b3b2e15",
   "metadata": {},
   "source": [
    "# Clustering results for each query $q_k \\in \\mathbb{R}^2$ where $k = 1, 2, 3$.\n",
    "\n",
    "I only consider the signs of the scores in this case. I used the second query set to produce the following results.\n",
    "\n",
    "\n",
    "Eyes + Eyebrows --> 0.7058\n",
    "\n",
    "Mouth --> 0.6078\n",
    "\n",
    "Eyes & Mouth --> 0.6470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b36635-3528-442f-b3e2-bf13dd237861",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = True\n",
    "X_eyes = (X[:, 0, :] > 0).astype(float)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_eyes)\n",
    "X_scaled = scaler.transform(X_eyes)\n",
    "\n",
    "if km:\n",
    "    k_means = KMeans(init=\"k-means++\", n_clusters=2, n_init=100, random_state=0)\n",
    "    y_hat = k_means.fit_predict(X_scaled, y)\n",
    "else:\n",
    "    clustering = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=1000).fit(X_scaled)\n",
    "    y_hat = clustering.fit_predict(X_scaled, y)\n",
    "\n",
    "acc = cluster_acc(y, y_hat)[0]\n",
    "print(\"Accuracy when using just eyes: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645356c-d77b-4353-9f5d-34f97d293bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = True\n",
    "X_mouth = (X[:, 1, :] > 0).astype(float)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_mouth)\n",
    "X_scaled = scaler.transform(X_mouth)\n",
    "\n",
    "if km:\n",
    "    k_means = KMeans(init=\"k-means++\", n_clusters=2, n_init=100, random_state=0)\n",
    "    y_hat = k_means.fit_predict(X_scaled, y)\n",
    "else:\n",
    "    clustering = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=1000).fit(X_scaled)\n",
    "    y_hat = clustering.fit_predict(X_scaled, y)\n",
    "\n",
    "acc = cluster_acc(y, y_hat)[0]\n",
    "print(\"Accuracy when using just mouth: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9894e2-6502-439e-bea4-3b0ad1ce4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = True\n",
    "X_em = (X[:, 2, :] > 0).astype(float)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_em)\n",
    "X_scaled = scaler.transform(X_em)\n",
    "\n",
    "if km:\n",
    "    k_means = KMeans(init=\"k-means++\", n_clusters=2, n_init=100, random_state=0)\n",
    "    y_hat = k_means.fit_predict(X_scaled, y)\n",
    "else:\n",
    "    clustering = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=1000).fit(X_scaled)\n",
    "    y_hat = clustering.fit_predict(X_scaled, y)\n",
    "\n",
    "acc = cluster_acc(y, y_hat)[0]\n",
    "print(\"Accuracy when using just eyes and mouth: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309089ab-0737-4c16-b9f7-685f8a17c72a",
   "metadata": {},
   "source": [
    "### Clustering results for each relevance vector  $ r \\in \\mathbb{R}^{3 \\times 2}$ (no dimensionality reduction)\n",
    "\n",
    "Here, I have only considered the signs of the relevance scores and no dimensionality reduction is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d206a40-b789-4598-8535-1d016e5e7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = (X.reshape(data_len, 3*2) > 0).astype(float)\n",
    "scaler = preprocessing.StandardScaler().fit(X_hat)\n",
    "X_scaled = scaler.transform(X_hat)\n",
    "\n",
    "k_means = KMeans(init=\"k-means++\", n_clusters=2, n_init=100, random_state=0)\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', max_iter=5000, init_params='k-means++',\n",
    "                  n_init=100, random_state=0).fit(X_hat)\n",
    "clustering = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=1000).fit(X_scaled)\n",
    "\n",
    "y_hat_kmeans = k_means.fit_predict(X_scaled, y)\n",
    "y_hat_clustering = clustering.fit_predict(X_scaled, y)\n",
    "y_hat_gmm = gmm.fit_predict(X_scaled, y)\n",
    "\n",
    "acc_kmeans = cluster_acc(y, y_hat_kmeans)[0]\n",
    "acc_clustering = cluster_acc(y, y_hat_clustering)[0]\n",
    "acc_gmm = cluster_acc(y, y_hat_gmm)[0]\n",
    "\n",
    "print(\"Accuracy achieved by K-means, when using just eyes and mouth: {}\".format(acc_kmeans))\n",
    "print(\"Accuracy achieved by Spectral Clustering, when using just eyes and mouth: {}\".format(acc_clustering))\n",
    "print(\"Accuracy achieved by GMM, when using just eyes and mouth: {}\".format(acc_gmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f987d1-dafa-4c03-9f9e-098b4497e3aa",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using T-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0ed47-db1a-4c5b-83ab-9f9cbfc4ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "X_bar = (X.reshape(data_len, 3*2) > 0).astype(float)\n",
    "tsne = TSNE(n_components=2, perplexity=3, n_iter=5000, random_state=0).fit_transform(X_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad2426-eb11-4f52-b66f-9d8840b55665",
   "metadata": {},
   "source": [
    "### Clustering results for each relevance vector  $ r \\in \\mathbb{R}^{3 \\times 2}$ (after dimensionality reduction)\n",
    "\n",
    "Perform clustering after dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6306a-3a0c-4b71-9e59-eabf2a58a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "\n",
    "X_hat = np.concatenate([tx.reshape(data_len, 1), ty.reshape(data_len, 1)], axis=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_hat)\n",
    "X_scaled = scaler.transform(X_hat)\n",
    "\n",
    "k_means = KMeans(init=\"k-means++\", n_clusters=2, n_init=100, random_state=0)\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', max_iter=5000, init_params='k-means++',\n",
    "                  n_init=100, random_state=0).fit(X_hat)\n",
    "clustering = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=1000).fit(X_scaled)\n",
    "\n",
    "y_hat_kmeans = k_means.fit_predict(X_scaled, y)\n",
    "y_hat_clustering = clustering.fit_predict(X_scaled, y)\n",
    "y_hat_gmm = gmm.fit_predict(X_scaled, y)\n",
    "\n",
    "acc_kmeans = cluster_acc(y, y_hat_kmeans)[0]\n",
    "acc_clustering = cluster_acc(y, y_hat_clustering)[0]\n",
    "acc_gmm = cluster_acc(y, y_hat_gmm)[0]\n",
    "\n",
    "print(\"Accuracy achieved by K-means, when using just eyes and mouth: {}\".format(acc_kmeans))\n",
    "print(\"Accuracy achieved by Spectral Clustering, when using just eyes and mouth: {}\".format(acc_clustering))\n",
    "print(\"Accuracy achieved by GMM, when using just eyes and mouth: {}\".format(acc_gmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933fdce-8646-4c9e-bf57-14ee35b54534",
   "metadata": {},
   "source": [
    "Perform dimensionality reduction and plot GMM solution on transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a085c78-7c45-4744-ac57-2e521b07c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "colors = sns.color_palette('tab10', n_colors=20)\n",
    "colors_per_class = {}\n",
    "\n",
    "for i, cls in enumerate([0, 6]):\n",
    "    colors_per_class[cls] = colors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9f6c6-7611-4b09-8113-d3c59c26db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm_solution(X, mu, sigma):\n",
    "    n_gaussians = mu.shape[0]\n",
    "    \n",
    "    # Plotting data points.\n",
    "    # for every class, we'll add a scatter plot separately\n",
    "    for label in colors_per_class.keys():\n",
    "        # find the samples of the current class in the data\n",
    "        indices = [i for i, l in enumerate(y) if l == label]\n",
    "     \n",
    "        # extract the coordinates of the points of this class only\n",
    "        current_tx = np.take(tx, indices)\n",
    "        current_ty = np.take(ty, indices)\n",
    "     \n",
    "        # convert the class color to matplotlib format\n",
    "        color = np.array([colors_per_class[label]])\n",
    "     \n",
    "        # add a scatter plot with the corresponding color and label\n",
    "        plt.scatter(current_tx, current_ty, c=color, label=label, alpha=0.7)\n",
    "    \n",
    "    # Plot ellipses and cluster centers\n",
    "    for i in range(n_gaussians):\n",
    "        plt.scatter(mu[i, 0], mu[i, 1], marker='X', color='r', s=100)\n",
    "        vals, vecs = np.linalg.eigh(sigma[i])\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "        vecs = vecs / np.linalg.norm(vecs)\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "        theta = 180 * theta / np.pi\n",
    "        width, height = 2 * np.sqrt(vals)\n",
    "        \n",
    "        elipse = matplotlib.patches.Ellipse(xy=[mu[i, 0], mu[i, 1]], width=width, height=height, angle=180+theta,\n",
    "              edgecolor='g', lw=4, facecolor='none')\n",
    "        plt.gcf().gca().add_artist(elipse)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b245b77-9e4c-411e-abca-96755482adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=2, covariance_type='full', max_iter=5000, init_params='k-means++',\n",
    "                      n_init=100, random_state=0).fit(X_hat)\n",
    "mu = gmm.means_\n",
    "sigma = gmm.covariances_\n",
    "plot_gmm_solution(X_hat, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db7251-9674-4806-ab19-cd53cb36a37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
