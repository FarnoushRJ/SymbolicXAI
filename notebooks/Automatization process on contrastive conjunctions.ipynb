{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc0f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d947e071-1b48-48a9-bdbc-111dcb59e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel path: /opt/homebrew/Caskroom/miniconda/base/envs/symbxai/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys; print(\"Python kernel path:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad22d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from symbxai.lrp.symbolic_xai import BERTSymbXAI\n",
    "from symbxai.model.transformer import  bert_base_uncased_model\n",
    "from dgl.data import SSTDataset\n",
    "\n",
    "from symbxai.visualization.query_search import plot_quali_table\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb99a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time, pickle, numpy\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# from html2image import Html2Image\n",
    "from symbxai.dataset.utils import load_sst_treebank\n",
    "\n",
    "from symbxai.query_search.utils import calc_weights\n",
    "\n",
    "from symbxai.query_search.utils import comp_all_harsanyi_sst, setup_queries, calc_attr_supp, calc_corr\n",
    "import multiprocessing\n",
    "from symbxai.dataset.utils import test_contr_conj, process_treeid2tokenid\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9805a1",
   "metadata": {},
   "source": [
    "# Get NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29226679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "sst_model = bert_base_uncased_model(\n",
    "        pretrained_model_name_or_path='textattack/bert-base-uncased-SST-2' )\n",
    "\n",
    "sst_model.eval()\n",
    "pretrained_embeddings = sst_model.bert.embeddings\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7974b",
   "metadata": {},
   "source": [
    "# Get SST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcf33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = SSTDataset(mode='train')\n",
    "# sst_dataset = load_sst_treebank([sample_id])\n",
    "vocab_words = list(tree_dataset.vocab.keys())\n",
    "\n",
    "# sample_range = [138, 259, ] # 1481, 4113\n",
    "\n",
    "# if False:\n",
    "result_str = ''\n",
    "count = 0\n",
    "sample_range = []\n",
    "for sample_id, tree_sample in enumerate(tree_dataset):\n",
    "    ccout = test_contr_conj(tree_sample, vocab_words, verbose=False)\n",
    "    \n",
    "    if ccout == False: \n",
    "        continue\n",
    "        \n",
    "    sentence = ccout[5]\n",
    "    lsent = ccout[4]\n",
    "    sample = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'].squeeze())\n",
    "\n",
    "    if len(tokens) > 20: continue\n",
    "    # print(sample_id, len(lsent))\n",
    "    count +=1\n",
    "    result_str += f'{sample_id} '\n",
    "    sample_range.append(sample_id)\n",
    "    \n",
    "\n",
    "sst_dataset = load_sst_treebank(sample_range, verbose=False)\n",
    "sst_dataset = sst_dataset['train']\n",
    "tree_dataset = SSTDataset(mode='train')\n",
    "vocab_words = list(tree_dataset.vocab.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cca2cac-15eb-49a7-9310-66322953c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few constrastive conjunction sentences\n",
      "138 259 324 385 413 658 851 1469 1518 1614 1716 2086 2351 2509 2555 3287 3423 3427 3493 3617 4328 4634 4724 5177 5293 5356 5602 5703 5734 6072 6347 6593 7147 7349 7411 7560 8015 8145\n"
     ]
    }
   ],
   "source": [
    "print('A few constrastive conjunction sentences')\n",
    "print(reduce( lambda x,y: str(x) + ' '+ str(y), list(sst_dataset['sentence'].keys())))\n",
    "# sample_range = [int(ns) for ns in '138 259 324 385 413 851 1469 1518 1532 1614 1716 2086 2555 3287 3423 3427 3437 3493 3617 4328 4634 4724 5177 5356 5676 5703 5734 6072 6347 6593 7349 7411 7560 8015 8145 8231'.split(' ')]\n",
    "# sample_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e921f9-6933-4c0d-941e-42c8055082e8",
   "metadata": {},
   "source": [
    "## Sample wise consideration of query contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4a17d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"strange it is, but delightfully so.\" prediction is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Harsanyi Dividends: 100%|██████████| 98/98 [00:55<00:00,  1.76it/s]\n",
      "Query weights for occlusion.: 100%|██████████| 98/98 [00:00<00:00, 1749114.01it/s]\n",
      "Query Attribution of corr(q,f).: 100%|██████████| 1300/1300 [00:00<00:00, 3461.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without paralyzation it took 0.376384973526001 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Custum parameter\n",
    "attribution_mode =   'corr(q,f)' #  'attribution'#\n",
    "weight_mode =   'occlusion' #  'significance-1' #  'shapley'   # \n",
    "# datamode=  'synthetic'# 'sst_treebank'  # 'sst_huggingface' # \n",
    "# neg_tokens_hars_bool= True\n",
    "neg_tokens = ['[CLS]', '[SEP]', ',', '.', '_', '-', \"'\" ]\n",
    "load_harsanyi = False\n",
    "load_queries = False\n",
    "neg_tokens_hars_bool = True\n",
    "parallelize  = False\n",
    "save_results = False\n",
    "# comp_subset_attr = False\n",
    "### \"strange it is, but delightfully so.\" --> sample 385\n",
    "### \"topics that could make a sailor blush - but lots of laughs.\" --> 1518\n",
    "sample_id =  385 # 1518\n",
    "sample_range = [sample_id]\n",
    "# sample_range = [int(ns) for ns in '138 259 324 385 413 851 1469 1518 1532 1614 1716 2086 2555 3287 3423 3427 3437 3493 3617 4328 4634 4724 5177 5356 5676 5703 5734 6072 6347 6593 7349 7411 7560 8015 8145 8231'.split(' ')]\n",
    "# hack = False\n",
    "####################################################################################\n",
    "# fix parameter\n",
    "harsanyi_maxorder=4\n",
    "max_and_order = 3\n",
    "max_indexdist = 1\n",
    "max_setsize =  float('inf')\n",
    "query_mode = 'conj. disj. (neg. disj.) reasonably mixed' # 'conj. disj. neg. reasonably mixed' # 'set conjuction'\n",
    "result_folder = 'intermediate_results/query_search_algo3/'\n",
    "###########################################################################################\n",
    "\n",
    "for sample_id in sample_range:\n",
    "    ####################################################################\n",
    "    tree_dataset = SSTDataset(mode='train')\n",
    "    sst_dataset = load_sst_treebank([sample_id])\n",
    "    vocab_words = list(tree_dataset.vocab.keys())\n",
    "    ## Process input\n",
    "    tree_sample = tree_dataset[sample_id]\n",
    "    try:\n",
    "        Xnids, Ynids, but_treeid, mask, lsent, sentence, sent_label, Xlabels, Ylabels = test_contr_conj(tree_sample, vocab_words, verbose=False)\n",
    "    except:\n",
    "        raise ValueError(f'sample {sample_id} is not a contrastive conjuction.')\n",
    "                         \n",
    "    assert sst_dataset['train']['sentence'][sample_id] == sentence\n",
    "    \n",
    "    harsdiv_name = f'harsanyi_div_maxorder-{harsanyi_maxorder}_sampleid-{sample_id}_datamode-sst_treebank.pkl'\n",
    "    queries_name = f'all_queries-{sample_id}_max_and_order-{max_and_order}_datamode-sst_treebank_attribution_mode-{attribution_mode}_query_mode-{query_mode}_harsanyi_maxorder-{harsanyi_maxorder}.pkl'\n",
    "    \n",
    "    sample = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'].squeeze())\n",
    "    \n",
    "    print(f'\"{sentence}\"', 'prediction is', sst_model(**sample)['logits'].argmax().item())\n",
    "    \n",
    "    neg_tokens_ids = [idn for idn, tok in enumerate(tokens) if tok in neg_tokens]\n",
    "    \n",
    "    if max_setsize > len(tokens):\n",
    "        max_setsize = len(tokens)\n",
    "        \n",
    "    ##################################################################################\n",
    "    # setup explainer\n",
    "    if sst_model(**sample)['logits'].argmax().item() == 0:\n",
    "        target = torch.tensor([1,-1])\n",
    "    else:\n",
    "        target = torch.tensor([-1,1])\n",
    "        \n",
    "    explainer = BERTSymbXAI(sample=sample,\n",
    "                            target=target,\n",
    "                            model=sst_model,\n",
    "                            embeddings=pretrained_embeddings)\n",
    "    ############################################################################################\n",
    "    ## Set up the explanation framework, \n",
    "    # 1) Harsanyi dividends\n",
    "    if load_harsanyi:\n",
    "        try:\n",
    "            hars_div = pickle.load(open(result_folder + harsdiv_name, 'rb'))\n",
    "        except:\n",
    "            print(f'could not load the Harsanyi dividends for {sample_id}')\n",
    "            continue\n",
    "    else:\n",
    "        hars_div = comp_all_harsanyi_sst(explainer, harsanyi_maxorder=harsanyi_maxorder, neg_tokens=None if not neg_tokens_hars_bool else neg_tokens_ids)\n",
    "        if save_results: pickle.dump(hars_div, open(result_folder + harsdiv_name, 'wb'))\n",
    "    \n",
    "    if load_queries:\n",
    "        try:\n",
    "            all_queries = pickle.load(open(result_folder + queries_name, 'rb'))\n",
    "        except:\n",
    "            print(f'could not load the queries for {sample_id}')\n",
    "            continue\n",
    "    else:\n",
    "        # 2) Queries\n",
    "        all_queries = setup_queries(explainer.node_domain, \n",
    "                                    tokens,\n",
    "                                    max_and_order, \n",
    "                                    max_setsize=max_setsize, \n",
    "                                    max_indexdist=max_indexdist, \n",
    "                                    mode=query_mode,\n",
    "                                    neg_tokens=neg_tokens_ids )\n",
    "        # 3) Weight function \n",
    "        weight_vec = calc_weights(weight_mode, hars_div, all_queries)\n",
    "        ##########################################################################################\n",
    "        \n",
    "        start = time.time()\n",
    "        if attribution_mode == 'corr(q,f)':\n",
    "            calculation_fct = calc_corr\n",
    "        else:\n",
    "             calculation_fct = calc_attr_supp\n",
    "        \n",
    "        if parallelize:\n",
    "            pool = multiprocessing.Pool(14)\n",
    "            args = [(q, hars_div, weight_vec) for q in all_queries]\n",
    "            all_queries = pool.map(calculation_fct, args)\n",
    "            pool.close()\n",
    "        else:\n",
    "            for q in tqdm(all_queries, desc=f'Query Attribution of {attribution_mode}.'):\n",
    "               q = calculation_fct((q, hars_div, weight_vec))\n",
    "                \n",
    "        print('with' if parallelize else 'without', f'paralyzation it took {time.time() - start} seconds.')\n",
    "        if save_results: pickle.dump(all_queries, open(result_folder + queries_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f493199-e26e-4983-93dc-d8c94a8ee4d7",
   "metadata": {},
   "source": [
    "# Look at some qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8e622e-df8b-4b58-a0e0-3513aa80637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52047 bytes written to file /Users/thomasschnake/Research/Projects/symbolic_xai_cleaned/symbolicXAI_github/notebooks/intermediate_results/query_search_algo/quali_table_search_exhaustive_sample-385_max_and_order-3_datamode-sst_treebank_querymode-conj. disj. (neg. disj.) reasonably mixed.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2> strange it is, but delightfully so. </h2><table ><tr><th style=\"text-align: center;\">occlusion</th></tr><tr><td style=\"font-family:Courier; color: white;text-align: center; opacity: 1; font-weight: bold;\" bgcolor=\"#ff3131\">&not;(is strange it) &wedge; delightful</td></tr><tr><td style=\"font-family:Courier; color: white;text-align: center; opacity: 1; font-weight: bold;\" bgcolor=\"#ff3434\">&not;##ly &wedge; delightful &wedge; &not;(is strange it)</td></tr><tr><td style=\"font-family:Courier; color: white;text-align: center; opacity: 1; font-weight: bold;\" bgcolor=\"#ff4646\">delightful &wedge; &not;(strange it)</td></tr><tr><td style=\"font-family:Courier; color: white;text-align: center; opacity: 1; font-weight: bold;\" bgcolor=\"#ff4646\">&not;##ly &wedge; delightful &wedge; &not;(strange it)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_top = 4\n",
    "nb_flop=0\n",
    "# pc_top = 100\n",
    "vismode = 'show all'\n",
    "fontcolor = 'white'\n",
    "file_str =  f'quali_table_search_exhaustive_sample-{sample_id}_max_and_order-{max_and_order}_datamode-sst_treebank_querymode-{query_mode}.png'\n",
    "all_outs = {weight_mode: {}}\n",
    "\n",
    "out_color_vals, _, out_keys = plot_quali_table(sentence, \n",
    "                                               tokens, \n",
    "                                               {weight_mode: all_queries}, \n",
    "                                               vismode, \n",
    "                                               nb_top=nb_top, \n",
    "                                               nb_flop=nb_flop, \n",
    "                                               file_str=file_str, \n",
    "                                               pc_top=100, \n",
    "                                               fontcolor=fontcolor\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d28ad3-43de-4880-b9a9-1fecfbbfa381",
   "metadata": {},
   "source": [
    "## Loading and evaluating the query contributinos from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87dde8-3959-4d83-a328-dd87f33a3134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1183f5c-ad51-4da7-a68d-14445bbe1f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t max_prese_Y \t max_absen_X \t min_absen_Y \t min_prese_X\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_prese_Y\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_absen_X\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_absen_Y\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_prese_X\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample_id \u001b[38;5;129;01min\u001b[39;00m sample_range:\n\u001b[0;32m---> 17\u001b[0m     tree_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSSTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     sst_dataset \u001b[38;5;241m=\u001b[39m load_sst_treebank([sample_id])\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m## Process input\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/tree.py:131\u001b[0m, in \u001b[0;36mSSTDataset.__init__\u001b[0;34m(self, mode, glove_embed_file, vocab_file, raw_dir, force_reload, verbose, transform)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSSTDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/dgl_dataset.py:333\u001b[0m, in \u001b[0;36mDGLBuiltinDataset.__init__\u001b[0;34m(self, name, url, raw_dir, hash_key, force_reload, verbose, transform)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDGLBuiltinDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/dgl_dataset.py:112\u001b[0m, in \u001b[0;36mDGLDataset.__init__\u001b[0;34m(self, name, url, raw_dir, save_dir, hash_key, force_reload, verbose, transform)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_dir \u001b[38;5;241m=\u001b[39m save_dir\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/dgl_dataset.py:190\u001b[0m, in \u001b[0;36mDGLDataset._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_flag:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone loading data from cached files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/tree.py:247\u001b[0m, in \u001b[0;36mSSTDataset.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    245\u001b[0m     emb_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trees \u001b[38;5;241m=\u001b[39m \u001b[43mload_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab \u001b[38;5;241m=\u001b[39m load_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_path)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pretrained_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/graph_serialize.py:195\u001b[0m, in \u001b[0;36mload_graphs\u001b[0;34m(filename, idx_list)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_graph_v1(filename, idx_list)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_graph_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid DGL Version Number.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/graph_serialize.py:207\u001b[0m, in \u001b[0;36mload_graph_v2\u001b[0;34m(filename, idx_list)\u001b[0m\n\u001b[1;32m    205\u001b[0m heterograph_list \u001b[38;5;241m=\u001b[39m _CAPI_LoadGraphFiles_V2(filename, idx_list)\n\u001b[1;32m    206\u001b[0m label_dict \u001b[38;5;241m=\u001b[39m load_labels_v2(filename)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [gdata\u001b[38;5;241m.\u001b[39mget_graph() \u001b[38;5;28;01mfor\u001b[39;00m gdata \u001b[38;5;129;01min\u001b[39;00m heterograph_list], label_dict\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/graph_serialize.py:207\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m heterograph_list \u001b[38;5;241m=\u001b[39m _CAPI_LoadGraphFiles_V2(filename, idx_list)\n\u001b[1;32m    206\u001b[0m label_dict \u001b[38;5;241m=\u001b[39m load_labels_v2(filename)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mgdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m gdata \u001b[38;5;129;01min\u001b[39;00m heterograph_list], label_dict\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/heterograph_serialize.py:66\u001b[0m, in \u001b[0;36mHeteroGraphData.get_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m eframes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ntid, ntensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ntensor_list):\n\u001b[0;32m---> 66\u001b[0m     ndict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m         ntensor[i]: F\u001b[38;5;241m.\u001b[39mzerocopy_from_dgl_ndarray(ntensor[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ntensor), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     69\u001b[0m     }\n\u001b[1;32m     70\u001b[0m     nframes\u001b[38;5;241m.\u001b[39mappend(Frame(ndict, num_rows\u001b[38;5;241m=\u001b[39mgidx\u001b[38;5;241m.\u001b[39mnum_nodes(ntid)))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m etid, etensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(etensor_list):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/data/heterograph_serialize.py:67\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m eframes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ntid, ntensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ntensor_list):\n\u001b[1;32m     66\u001b[0m     ndict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 67\u001b[0m         ntensor[i]: F\u001b[38;5;241m.\u001b[39mzerocopy_from_dgl_ndarray(\u001b[43mntensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ntensor), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     69\u001b[0m     }\n\u001b[1;32m     70\u001b[0m     nframes\u001b[38;5;241m.\u001b[39mappend(Frame(ndict, num_rows\u001b[38;5;241m=\u001b[39mgidx\u001b[38;5;241m.\u001b[39mnum_nodes(ntid)))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m etid, etensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(etensor_list):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/container.py:32\u001b[0m, in \u001b[0;36mList.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     29\u001b[0m         stop \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, stop, step)]\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList index out of range. List size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got index \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m), i\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/container.py:46\u001b[0m, in \u001b[0;36mList.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_api_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ListSize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/_ffi/_ctypes/function.py:209\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the function with positional arguments\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03margs : list\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m   The positional arguments to the function call.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 209\u001b[0m values, tcodes, num_args \u001b[38;5;241m=\u001b[39m \u001b[43m_make_dgl_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DGLValue()\n\u001b[1;32m    211\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/symbxai/lib/python3.10/site-packages/dgl/_ffi/_ctypes/function.py:118\u001b[0m, in \u001b[0;36m_make_dgl_args\u001b[0;34m(args, temp_args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ObjectBase):\n\u001b[1;32m    117\u001b[0m     values[i]\u001b[38;5;241m.\u001b[39mv_handle \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m--> 118\u001b[0m     type_codes[i] \u001b[38;5;241m=\u001b[39m TypeCode\u001b[38;5;241m.\u001b[39mOBJECT_HANDLE\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m, ObjectGeneric)):\n\u001b[1;32m    120\u001b[0m     arg \u001b[38;5;241m=\u001b[39m convert_to_object(arg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attribution_mode =  'cov(q,f)'  #  'attribution'#\n",
    "weight_mode =   'occlusion' #  'significance-1' #  'shapley'   # \n",
    "query_mode = 'conj. disj. (neg. disj.) reasonably mixed' \n",
    "results_dir = 'intermediate_results/query_search_algo3/'\n",
    "harsanyi_maxorder = 4\n",
    "# neg_ids = [1614, 1716, 3437, 5703, 8015]\n",
    "verbose = True\n",
    "# sample_range = [ids for ids in sample_range if ids not in neg_ids]\n",
    "# sample_range = [385]\n",
    "sample_range = [int(ns) for ns in '138 259 324 385 413 851 1469 1518 1532 1614 1716 2086 2555 3287 3423 3427 3437 3493 3617 4328 4634 4724 5177 5356 5676 5703 5734 6072 6347 6593 7349 7411 7560 8015 8145 8231'.split(' ')]\n",
    "nb_max = 1\n",
    "nb_min = 1\n",
    "\n",
    "bool_outs = {}\n",
    "print('\\t\\t','max_prese_Y','\\t', 'max_absen_X','\\t', 'min_absen_Y', '\\t','min_prese_X')\n",
    "for sample_id in sample_range:\n",
    "    tree_dataset = SSTDataset(mode='train')\n",
    "    sst_dataset = load_sst_treebank([sample_id])\n",
    "    ## Process input\n",
    "    tree_sample = tree_dataset[sample_id]\n",
    "    \n",
    "    try:\n",
    "        Xnids, Ynids, but_treeid, mask, lsent, sentence, sent_label, Xlabels, Ylabels = test_contr_conj(tree_sample, vocab_words, verbose=False)\n",
    "    except:\n",
    "        raise ValueError(f'sample {sample_id} is not a contrastive conjuction.')\n",
    "                         \n",
    "    assert sst_dataset['train']['sentence'][sample_id] == sentence\n",
    "    \n",
    "    queries_name = f'all_queries-{sample_id}_max_and_order-3_datamode-sst_treebank_attribution_mode-{attribution_mode}_query_mode-{query_mode}_harsanyi_maxorder-{harsanyi_maxorder}.pkl'\n",
    "    try:\n",
    "        all_queries = pickle.load(open(results_dir + queries_name, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        if verbose: print('no file for:', sample_id)\n",
    "        continue\n",
    "    \n",
    "    sample = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'].squeeze())\n",
    "\n",
    "    # print('we loaded', sample_id)\n",
    "\n",
    "    \n",
    "    all_attributions = {q.hash: q.attribution for q in all_queries}\n",
    "    all_str_rep = {q.hash: q.str_rep for q in all_queries}\n",
    "    queries_sorted = sorted(all_attributions, key=all_attributions.get)[::-1]\n",
    "    \n",
    "    max_hashs = queries_sorted[:nb_max]\n",
    "    min_hashs = queries_sorted[-nb_min:]\n",
    "    print(sentence)\n",
    "    try:\n",
    "        \n",
    "        Xtokenids = process_treeid2tokenid(Xnids, mask, tokens, lsent, verbose=False)\n",
    "        Ytokenids =  process_treeid2tokenid(Ynids, mask, tokens, lsent, verbose=False)\n",
    "        but_tokenids = process_treeid2tokenid([but_treeid], mask, tokens, lsent, verbose=False)\n",
    "    except RuntimeError:\n",
    "        print('parsing not possible for', sample_id)\n",
    "        continue\n",
    "            \n",
    "    max_prese_Y = False\n",
    "    max_absen_X = False\n",
    "    min_absen_Y = False\n",
    "    min_prese_X = False\n",
    "    # if verbose:\n",
    "    #     print('Xfeats are', [tokens[idf] for idf in Xtokenids])\n",
    "    #     print('Yfeats are', [tokens[idf] for idf in Ytokenids])\n",
    "    #     print('but_feats are', [tokens[idf] for idf in but_tokenids])\n",
    "        \n",
    "    for hash in max_hashs:\n",
    "        print('max hash')\n",
    "        display(HTML(all_str_rep[hash]))\n",
    "        for feat_set in hash:\n",
    "            # if verbose: print('feat in max_hash', [tokens[idf] for idf in feat_set])\n",
    "            \n",
    "            # check if the query finds tokens in Y important\n",
    "            if bool(set(Ytokenids).intersection(feat_set)):\n",
    "                max_prese_Y = True\n",
    "            \n",
    "            # check if the query finds tokens the negation of X important\n",
    "            if bool(set([tid - len(tokens) for tid in Xtokenids]).intersection(feat_set)):\n",
    "                max_absen_X = True\n",
    "    \n",
    "    for hash in min_hashs:\n",
    "        print('min hash')\n",
    "        display(HTML(all_str_rep[hash]))\n",
    "        for feat_set in hash:\n",
    "            # if verbose: print('feat in min_hash', [tokens[idf] for idf in feat_set])\n",
    "            # check if the query finds the abcenΩce of Y important\n",
    "            if bool(set([tid - len(tokens) for tid in Ytokenids]).intersection(feat_set)):\n",
    "                min_absen_Y = True\n",
    "                \n",
    "            # check if the query finds the precence of X important\n",
    "            if bool(set(Xtokenids).intersection(feat_set)):\n",
    "                min_prese_X = True\n",
    "    bool_outs[sample_id] = (max_prese_Y, max_absen_X, min_absen_Y, min_prese_X)\n",
    "    \n",
    "    print('id:',sample_id, '\\t',max_prese_Y,'\\t\\t', max_absen_X,'\\t\\t', min_absen_Y, '\\t\\t',min_prese_X)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb135e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbxai",
   "language": "python",
   "name": "symbxai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
